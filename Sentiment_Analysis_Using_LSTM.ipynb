{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T02:39:52.681281Z",
     "iopub.status.busy": "2025-05-21T02:39:52.680547Z",
     "iopub.status.idle": "2025-05-21T02:39:52.688962Z",
     "shell.execute_reply": "2025-05-21T02:39:52.688259Z",
     "shell.execute_reply.started": "2025-05-21T02:39:52.681246Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T02:39:54.850541Z",
     "iopub.status.busy": "2025-05-21T02:39:54.849993Z",
     "iopub.status.idle": "2025-05-21T02:40:05.643094Z",
     "shell.execute_reply": "2025-05-21T02:40:05.642219Z",
     "shell.execute_reply.started": "2025-05-21T02:39:54.850519Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) - {\"not\", \"no\", \"nor\"}\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\")\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "df['cleaned'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Tokenization and Padding\n",
    "MAX_LEN = 200\n",
    "MAX_VOCAB = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['cleaned'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['cleaned'])\n",
    "X = pad_sequences(X, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T02:48:52.473242Z",
     "iopub.status.busy": "2025-05-21T02:48:52.472933Z",
     "iopub.status.idle": "2025-05-21T02:51:25.297637Z",
     "shell.execute_reply": "2025-05-21T02:51:25.296733Z",
     "shell.execute_reply.started": "2025-05-21T02:48:52.473223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 27ms/step - accuracy: 0.5110 - loss: 0.6931 - val_accuracy: 0.5262 - val_loss: 0.6881\n",
      "Epoch 2/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.6219 - loss: 0.6352 - val_accuracy: 0.8294 - val_loss: 0.3875\n",
      "Epoch 3/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.8729 - loss: 0.3279 - val_accuracy: 0.8864 - val_loss: 0.2895\n",
      "Epoch 4/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9303 - loss: 0.1974 - val_accuracy: 0.8832 - val_loss: 0.3119\n",
      "Epoch 5/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9598 - loss: 0.1311 - val_accuracy: 0.8851 - val_loss: 0.3596\n",
      "Epoch 6/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9780 - loss: 0.0843 - val_accuracy: 0.8766 - val_loss: 0.3856\n",
      "Epoch 7/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9821 - loss: 0.0682 - val_accuracy: 0.8824 - val_loss: 0.4278\n",
      "Epoch 8/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9885 - loss: 0.0491 - val_accuracy: 0.8739 - val_loss: 0.5267\n",
      "Epoch 9/20\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 26ms/step - accuracy: 0.9910 - loss: 0.0398 - val_accuracy: 0.8742 - val_loss: 0.5888\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(MAX_VOCAB, 128, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(64, return_sequences=True)),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_lstm_model_finetuned.h5\", save_best_only=True, monitor=\"val_accuracy\", mode=\"max\")\n",
    "early_stop = EarlyStopping(patience=6, monitor=\"val_loss\", restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint, early_stop]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T02:51:32.533582Z",
     "iopub.status.busy": "2025-05-21T02:51:32.532846Z",
     "iopub.status.idle": "2025-05-21T02:51:32.724633Z",
     "shell.execute_reply": "2025-05-21T02:51:32.723841Z",
     "shell.execute_reply.started": "2025-05-21T02:51:32.533556Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tokenizer_finetuned.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T02:51:35.413951Z",
     "iopub.status.busy": "2025-05-21T02:51:35.413274Z",
     "iopub.status.idle": "2025-05-21T02:51:36.356466Z",
     "shell.execute_reply": "2025-05-21T02:51:36.355823Z",
     "shell.execute_reply.started": "2025-05-21T02:51:35.413927Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 417ms/step\n",
      "Sentiment: Positive (Confidence: 0.9200000166893005)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) - {\"not\", \"no\", \"nor\"}\n",
    "\n",
    "MAX_LEN = 200\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load model & tokenizer\n",
    "model = load_model(\"best_lstm_model_finetuned.h5\")\n",
    "with open(\"tokenizer_finetuned.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Predict function\n",
    "def predict_sentiment(text):\n",
    "    cleaned = clean_text(text)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(seq, maxlen=MAX_LEN, padding='post')\n",
    "    pred = model.predict(padded)[0][0]\n",
    "    label = \"Positive\" if pred >= 0.5 else \"Negative\"\n",
    "    confidence = pred if pred >= 0.5 else 1 - pred\n",
    "    return label, round(confidence, 2)\n",
    "\n",
    "sample_text = \"I absolutely loved the movie! Brilliant acting and direction.\"\n",
    "label, confidence = predict_sentiment(sample_text)\n",
    "print(f\"Sentiment: {label} (Confidence: {confidence})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
